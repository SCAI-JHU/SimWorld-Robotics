# SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration
<p align="center">
  <img src="images/overview.png" alt="overview" width="800">
</p>

**SimWorld-Robotics** is an extension of [SimWorld](https://github.com/SimWorld-AI/SimWorld) that introduces key features for embodied robotics research. These additions include procedural city generation, a traffic system, and support for a new embodied agent: the quadruped robot.

<!-- <div align="center">
    <a href="https://simworld-ai.github.io/"><img src="https://img.shields.io/badge/Website-SimWorld-blue" alt="Website" /></a>
</div> -->

## ðŸ”¥ News
 - 2025.10 The first formal release of **SimWorld-Robotics** has been published! ðŸš€
 - 2025.9 **SimWorld-Robotics** has been accepted to NeurIPS 2025 main track! ðŸŽ‰


## ðŸ’¡ Introduction
This repo serves as a benchmark platform for the SimWorld-MMNav and SimWorld-MRS in **SimWorld-Robotics**:

1.  A standardized OpenAI gym interface for connecting and evaluating various baselines.
2.  Procedural scene and task generation for creating diverse and scalable simulation environments and tasks.
3.  The SimWorld-20k dataset is available via this [link](https://huggingface.co/datasets/jise/simworld-20k).


### Project Structure
```bash
SimWorld-Robotics/
â”œâ”€â”€ baseline/                     # Baselines for SimWorld-MMNav and SimWorld-MRS
â”‚   â”œâ”€â”€ requirements.txt          # Baseline dependencies
â”‚   â”œâ”€â”€ single/                   # Single-agent navigation baselines
â”‚   â”œâ”€â”€ multi/                    # Multi-agent collaboration baselines
â”‚   â”œâ”€â”€ finetune/                 # Fine-tuning scripts
â”‚   â””â”€â”€ vla/                      # Vision-Language-Action models
â”œâ”€â”€ simworld_gym/                 # Core gym environment package
â”‚   â”œâ”€â”€ config/                   # Configuration files for assets and robots
â”‚   â”œâ”€â”€ envs/                     # Gym environments
â”‚   â”‚   â”œâ”€â”€ simple_world.py       # Single-agent navigation environment
â”‚   â”‚   â”œâ”€â”€ traffic_world.py      # Single-agent with traffic
â”‚   â”‚   â”œâ”€â”€ world_buffer.py       # Multi-agent environment
â”‚   â”‚   â””â”€â”€ setting/              # Task data (extracted from test_data)
â”‚   â”‚       â”œâ”€â”€ single_agent_world/  # Single-agent tasks
â”‚   â”‚       â””â”€â”€ multi_agent_world/   # Multi-agent tasks
â”‚   â”œâ”€â”€ task_generator/           # Procedural task generation
â”‚   â””â”€â”€ utils/                    # Utility functions
â”œâ”€â”€ sample_baseline.ipynb         # Quick start notebook for single-agent navigation
â”œâ”€â”€ video_record.ipynb            # Video recording utility
â””â”€â”€ readme.md                     # Me
```

## ðŸš€ Setup

### Installation

1. **Clone the repository**
```bash
git clone git@github.com:SCAI-JHU/SimWorld-Robotics.git
cd SimWorld-Robotics
```

2. **Install the core gym environment**
```bash
cd simworld_gym
pip install -e .
cd ..
```

3. **Install baseline dependencies**
```bash
pip install -r baseline/requirements.txt
```

4. **Download and Extract test data**

- Extract `single_test.tar.gz` to `simworld_gym/envs/setting/single_agent_world/`
- Extract `multi_test.tar.gz` to `simworld_gym/envs/setting/multi_agent_world/`

### Quick Start with Sample Baseline

Try out a single-agent navigation task using the interactive notebook: `sample_baseline.ipynb`

This notebook demonstrates:
- Loading a sample navigation task
- Running a vision-language model agent
- Visualizing navigation results
